{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Least Squares Practice Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will walkthrough using genearlized least squares (GLS) to analyze data and why it is useful method in analyzing data. \n",
    "\n",
    "For this assignment we will use global warming data from the r package `faraway`. This data set contains the Average Northen Hemisphere Temperature from 1856-2000 and eight climate proxies from 1000-2000AD. \n",
    "\n",
    "To begin, we will analyze the data using ordinary least squares (OLS). OLS assumes errors in the regression model have the same variance and are uncorrelated. Let's see if this is a valid assumption for our data!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "load('globwarm.rda')\n",
    "globwarm <- na.omit(globwarm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will use `lm()` to fit a linear model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmod <- lm(nhtemp ~ wusa + jasper + westgreen + chesapeake + tornetrask + urals + mongolia + tasman, data = globwarm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `residuals()` to measure the error between the predicted value of our model and the observed value, and use `cor()` to see if our errors are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.583338985012419"
      ],
      "text/latex": [
       "0.583338985012419"
      ],
      "text/markdown": [
       "0.583338985012419"
      ],
      "text/plain": [
       "[1] 0.583339"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find the number of observations\n",
    "n <- length(residuals(lmod)) \n",
    "\n",
    "# measures between the first and last observations\n",
    "cor(residuals(lmod)[-1], residuals(lmod)[-n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the errors are correlated. A proxy to see how the errors behave is to look at the residuals. Looking at the correlation between the residuals lagged by one year. Years 2 to n and years 1 to (-n). Taking the correlation between the year and the subsequent years residuals.\n",
    "\n",
    "Therefore fitting normal linear model will violate the correlation errors assumption. We will now use GLS to model the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the lecture on GLS that we need to estimate big Sigma. This will tell us the structure we are expecting from the correlation between these errors. We will then use that to be able to fit a model that incorporates that correlation appropriately. \n",
    "\n",
    "Assuming the errors take a simple auto-regressive form such that each error is correlated with the prior:\n",
    "\n",
    "$$ \\epsilon_{i+1} = \\phi\\epsilon_i + \\delta_i$$ such that, $$\\delta_i \\sim N(0, \\tau^2)$$ \n",
    "\n",
    "$\\phi$ is going to be the correlation coefficient and we have it added to an additional error term $\\delta_i$ we expect since it is normally distributed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimated $\\phi$ above using `cor(residuals(lmod)[-1], residuals(lmod)[-n])`, which is the correlation we are estimating between the i^th error and the i^th+1 error an we got the value of 0.583339"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the assumption $$\\Sigma_{ij} =  \\phi^{|i-j|}$$ we can estimate it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X <- model.matrix(lmod)  # design matrix \n",
    "\n",
    "Sigma <- diag(n) # I want a matrix that is nxn. 0 on off diagonals and 1 on the diagonals. \n",
    "Sigma <- 0.5833^abs(row(Sigma)- col(Sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A matrix: 9 × 1 of type dbl</caption>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>(Intercept)</th><td>-0.234134783</td></tr>\n",
       "\t<tr><th scope=row>wusa</th><td> 0.068425906</td></tr>\n",
       "\t<tr><th scope=row>jasper</th><td>-0.218438446</td></tr>\n",
       "\t<tr><th scope=row>westgreen</th><td> 0.003880871</td></tr>\n",
       "\t<tr><th scope=row>chesapeake</th><td>-0.014952072</td></tr>\n",
       "\t<tr><th scope=row>tornetrask</th><td> 0.057691347</td></tr>\n",
       "\t<tr><th scope=row>urals</th><td> 0.222078555</td></tr>\n",
       "\t<tr><th scope=row>mongolia</th><td> 0.055247801</td></tr>\n",
       "\t<tr><th scope=row>tasman</th><td> 0.122999856</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 9 × 1 of type dbl\n",
       "\\begin{tabular}{r|l}\n",
       "\t(Intercept) & -0.234134783\\\\\n",
       "\twusa &  0.068425906\\\\\n",
       "\tjasper & -0.218438446\\\\\n",
       "\twestgreen &  0.003880871\\\\\n",
       "\tchesapeake & -0.014952072\\\\\n",
       "\ttornetrask &  0.057691347\\\\\n",
       "\turals &  0.222078555\\\\\n",
       "\tmongolia &  0.055247801\\\\\n",
       "\ttasman &  0.122999856\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 9 × 1 of type dbl\n",
       "\n",
       "| (Intercept) | -0.234134783 |\n",
       "| wusa |  0.068425906 |\n",
       "| jasper | -0.218438446 |\n",
       "| westgreen |  0.003880871 |\n",
       "| chesapeake | -0.014952072 |\n",
       "| tornetrask |  0.057691347 |\n",
       "| urals |  0.222078555 |\n",
       "| mongolia |  0.055247801 |\n",
       "| tasman |  0.122999856 |\n",
       "\n"
      ],
      "text/plain": [
       "            [,1]        \n",
       "(Intercept) -0.234134783\n",
       "wusa         0.068425906\n",
       "jasper      -0.218438446\n",
       "westgreen    0.003880871\n",
       "chesapeake  -0.014952072\n",
       "tornetrask   0.057691347\n",
       "urals        0.222078555\n",
       "mongolia     0.055247801\n",
       "tasman       0.122999856"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y <- globwarm$nhtemp\n",
    "\n",
    "Sigma_inv <- solve(Sigma)\n",
    "XTX_inv <- solve(t(X)%*% Sigma_inv %*% X)\n",
    "betahat <- XTX_inv %*% t(X) %*% Sigma_inv %*% y # class derivation \n",
    "betahat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the correlation now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.588777640716299"
      ],
      "text/latex": [
       "0.588777640716299"
      ],
      "text/markdown": [
       "0.588777640716299"
      ],
      "text/plain": [
       "[1] 0.5887776"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res  <- y - X%*% betahat\n",
    "cor(res[-1], res[-n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method is to use the P matrix we derived during the lecture: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Py ~ PX - 1)\n",
       "\n",
       "Coefficients:\n",
       "PX(Intercept)         PXwusa       PXjasper    PXwestgreen   PXchesapeake  \n",
       "    -0.234135       0.068426      -0.218438       0.003881      -0.014952  \n",
       " PXtornetrask        PXurals     PXmongolia       PXtasman  \n",
       "     0.057691       0.222079       0.055248       0.123000  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "P <- chol(Sigma)  # square root of sigma matrix / choleski decomposition \n",
    "P_inv <- solve(t(P))\n",
    "\n",
    "PX <- P_inv %*% X\n",
    "Py <- P_inv %*% y \n",
    "\n",
    "lm(Py ~ PX -1) # -1 is because PX has an intercept in it and I don't want it to fix in that additional intercept. He we regressed the Prime values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `nlme` package in R has a function to perform GLS: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generalized least squares fit by REML\n",
       "  Model: nhtemp ~ wusa + jasper + westgreen + chesapeake + tornetrask +      urals + mongolia + tasman \n",
       "  Data: globwarm \n",
       "        AIC       BIC   logLik\n",
       "  -108.2074 -76.16822 65.10371\n",
       "\n",
       "Correlation Structure: AR(1)\n",
       " Formula: ~year \n",
       " Parameter estimate(s):\n",
       "      Phi \n",
       "0.7109922 \n",
       "\n",
       "Coefficients:\n",
       "                  Value  Std.Error   t-value p-value\n",
       "(Intercept) -0.23010624 0.06702406 -3.433188  0.0008\n",
       "wusa         0.06673819 0.09877211  0.675678  0.5004\n",
       "jasper      -0.20244335 0.18802773 -1.076668  0.2835\n",
       "westgreen   -0.00440299 0.08985321 -0.049002  0.9610\n",
       "chesapeake  -0.00735289 0.07349791 -0.100042  0.9205\n",
       "tornetrask   0.03835169 0.09482515  0.404446  0.6865\n",
       "urals        0.24142199 0.22871028  1.055580  0.2930\n",
       "mongolia     0.05694978 0.10489786  0.542907  0.5881\n",
       "tasman       0.12034918 0.07456983  1.613913  0.1089\n",
       "\n",
       " Correlation: \n",
       "           (Intr) wusa   jasper wstgrn chespk trntrs urals  mongol\n",
       "wusa       -0.517                                                 \n",
       "jasper     -0.058 -0.299                                          \n",
       "westgreen   0.330 -0.533  0.121                                   \n",
       "chesapeake  0.090 -0.314  0.230  0.147                            \n",
       "tornetrask -0.430  0.499 -0.197 -0.328 -0.441                     \n",
       "urals      -0.110 -0.142 -0.265  0.075 -0.064 -0.346              \n",
       "mongolia    0.459 -0.437 -0.205  0.217  0.449 -0.343 -0.371       \n",
       "tasman      0.037 -0.322  0.065  0.134  0.116 -0.434  0.416 -0.017\n",
       "\n",
       "Standardized residuals:\n",
       "        Min          Q1         Med          Q3         Max \n",
       "-2.31122523 -0.53484054  0.02342908  0.50015642  2.97224724 \n",
       "\n",
       "Residual standard error: 0.204572 \n",
       "Degrees of freedom: 145 total; 136 residual"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(nlme)\n",
    "\n",
    "glmod <- gls(nhtemp ~ wusa + jasper + westgreen + chesapeake + tornetrask + urals + mongolia + tasman, correlation = corAR1(form = ~ year), data = globwarm)\n",
    "\n",
    "summary(glmod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see `Phi` is estimated to be 0.7109922. \n",
    "\n",
    "This is for a one year lag which is a pretty high correlation but make sense in terms of year to year temperature. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate the confidence interval on that using the `intervals` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Approximate 95% confidence intervals\n",
       "\n",
       " Correlation structure:\n",
       "        lower      est.     upper\n",
       "Phi 0.5099658 0.7109922 0.8383787\n",
       "attr(,\"label\")\n",
       "[1] \"Correlation structure:\"\n",
       "\n",
       " Residual standard error:\n",
       "    lower      est.     upper \n",
       "0.1540697 0.2045720 0.2716284 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "intervals(glmod, which = \"var-cov\") # conf-int for the phi value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Safe to say to that there is definitely some correlation that is occurring between these errors so we did the right thing to fit a GLS model for this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
